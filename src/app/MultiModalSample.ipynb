{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal LLM Sample\n",
    "\n",
    "## Azure AI Vision\n",
    "\n",
    "[Azure AI Vision](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview) offers a powerful multi-modal [embedding functionality](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/image-retrieval). It generates numerical representations (vectors) for images, capturing their features and context and ***preserving the semantic meaning of the image***.\n",
    "\n",
    "These vectors allow ***text-based search*** over the same vector space. By converting both images and text queries into vectors, it enables semantic matching between images and textual descriptions. Check [this](../../README.md) link for further information.\n",
    "\n",
    "## Step 1: Install Nuget Packages & Read App Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.Search.Documents, 11.5.0-beta.4</span></li><li><span>Azure.Storage.Blobs, 12.19.0</span></li><li><span>DotNetEnv, 2.5.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuget packages installed...\n",
      "Configuration file loaded...\n"
     ]
    }
   ],
   "source": [
    "#r \"nuget: DotNetEnv, 2.5.0\"\n",
    "#r \"nuget: Azure.Search.Documents, 11.5.0-beta.4\"\n",
    "#r \"nuget: Azure.Storage.Blobs, 12.19.0\"\n",
    "\n",
    "using System.IO;\n",
    "using DotNetEnv;\n",
    "\n",
    "string configurationFile = \"../conf/configuration.env\";\n",
    "Env.Load(configurationFile);\n",
    "\n",
    "string visionApiKey = Env.GetString(\"AI.VISION.APIKEY\");\n",
    "string visionEndPoint = Env.GetString(\"AI.VISION.ENDPOINT\");\n",
    "string searchApiKey = Env.GetString(\"AI.SEARCH.APIKEY\");\n",
    "string searchEndpoint = Env.GetString(\"AI.SEARCH.ENDPOINT\");\n",
    "string storageConnectionString = Env.GetString(\"AI.STORAGE.CONNECTIONSTRING\");\n",
    "\n",
    "if (!File.Exists(configurationFile)) {\n",
    "  Console.WriteLine(\"Configuration file does not exist. Pleae execute AZ CLI script provided in ../create_env/\"); \n",
    "} else {\n",
    "  Console.WriteLine(\"Nuget packages installed...\");\n",
    "  Console.WriteLine(\"Configuration file loaded...\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Helper Class\n",
    "\n",
    "The `AiVisionHelper` [class](AiVisionHelper.csx) provides simplified abstractions to:\n",
    "  - Create an image embedding by crafting and executing a http call to the AI Vision REST endpoint.\n",
    "  - Create an text embedding by crafting and executing a http call to the AI Vision REST endpoint.\n",
    "  - Upload images to Azure Blob storage and creating a SAS\n",
    "  - Create a search index in Azure AI Search\n",
    "  - Store embeddings in Azure AI Search\n",
    "  - Query Azure AI Search based on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class AiVisionHelper with methods:\n",
      "- GetTextEmbedding(string text)\n",
      "- GetImageEmbedding(string imageUrl)\n",
      "- UploadLocalFile(string localFilePath, string containerName, string blobName)\n",
      "- CreateSearchIndex(string indexName)\n",
      "- StoreImageEmbedding(string indexName, List<ImageEmbedding> imageEmbeddings)\n",
      "- QuerySearchIndex(string indexName, float[] queryVector)\n",
      "created...\n"
     ]
    }
   ],
   "source": [
    "#!import \"AiVisionHelper.csx\"\n",
    "\n",
    "Console.WriteLine($\"Class AiVisionHelper with methods:\");\n",
    "Console.WriteLine($\"- GetTextEmbedding(string text)\");\n",
    "Console.WriteLine($\"- GetImageEmbedding(string imageUrl)\");\n",
    "Console.WriteLine($\"- UploadLocalFile(string localFilePath, string containerName, string blobName)\");\n",
    "Console.WriteLine($\"- CreateSearchIndex(string indexName)\");\n",
    "Console.WriteLine($\"- StoreImageEmbedding(string indexName, List<ImageEmbedding> imageEmbeddings)\");\n",
    "Console.WriteLine($\"- QuerySearchIndex(string indexName, float[] queryVector)\");\n",
    "Console.WriteLine($\"created...\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create and store image embeddings\n",
    "\n",
    "The code cell uses the above defined `AiVisionHelper` class to:\n",
    "\n",
    "- Create an Azure AI Search search index (`aiVisionHelper.CreateSearchIndes()`)\n",
    "- Upload architecture sketches to Azure Blob storage (`aiVisionHelper.UploadLocalFile()`) and create a Shared Access Signature for the uploaded architecture sketch.\n",
    "- Create image embeddings using Azure AI Vision (`aiVisionHelper.GetImageEmbedding()` by providing the previously created SAS). This will be executed in a loop for all existing architecture sketches. (Image name & image embedding are stored in a Dictionary `embeddings`)\n",
    "- Store image name and image vector in Azure AI Search (`aiVisionHelper.StoreImageEmbedding()`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure AI Search index created...\n",
      "Azure AI Vision embeddings created...\n",
      "Azure AI Search embeddings stored...\n"
     ]
    }
   ],
   "source": [
    "//Azure AI Search - Create Search Index\n",
    "string searchIndexName = \"architecture\";\n",
    "\n",
    "AiVisionHelper _aiVisionHelper = new AiVisionHelper(visionApiKey, visionEndPoint, searchApiKey, searchEndpoint, storageConnectionString);\n",
    "await _aiVisionHelper.CreateSearchIndex(searchIndexName);\n",
    "Console.WriteLine(\"Azure AI Search index created...\");\n",
    "\n",
    "//Azure AI Vision - Get Image Embedding\n",
    "Dictionary<string, float[]> embeddings = new Dictionary<string, float[]>(); \n",
    "\n",
    "foreach(string fileName in Directory.GetFiles(\"../../media/architecture\", \"*.png\"))\n",
    "{\n",
    "    Uri sasUri = await _aiVisionHelper.UploadLocalFile(fileName, \"architecture\", Path.GetFileName(fileName));\n",
    "    embeddings.Add(fileName, await _aiVisionHelper.GetImageEmbedding(sasUri.ToString()));\n",
    "}\n",
    "Console.WriteLine(\"Azure AI Vision embeddings created...\");\n",
    "\n",
    "//Azure AI Search - Store Image Embedding\n",
    "List<ImageEmbedding> imageEmbeddings = new List<ImageEmbedding>();\n",
    "foreach(KeyValuePair<string, float[]> embedding in embeddings)\n",
    "{\n",
    "    imageEmbeddings.Add(new ImageEmbedding() { \n",
    "        ImageId = Guid.NewGuid().ToString(),\n",
    "        ImageDescription = embedding.Key, \n",
    "        ImageVector = embedding.Value \n",
    "    });\n",
    "}\n",
    "await _aiVisionHelper.StoreImageEmbedding(searchIndexName, imageEmbeddings);\n",
    "Console.WriteLine(\"Azure AI Search embeddings stored...\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Search Index\n",
    "\n",
    "The Azure AI Search search index is queried using the search word ***Vector***. The Azure AI Vision MLLM which was used to calculate the image embeddings will be used this time to calculate a text embedding for \"Vector\". The helper function `aiVisionHelper.GetTextEmbedding()` is used to create the embedding. \n",
    "\n",
    "To query Azure AI Search the helper method `aiVisionHelper.QuerySearchIndex()` is used and the search index name which was used to store the image embeddings and the calculated text embedding are provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cognitive Search Results for 'Vector':\n",
      "  - ../../media/architecture\\VectorEmbedding.png\n"
     ]
    }
   ],
   "source": [
    "string question = \"Vector\";\n",
    "\n",
    "float[] queryVector = await _aiVisionHelper.GetTextEmbedding(question);\n",
    "\n",
    "//Cognitive Search - Query Search Index\n",
    "List<ImageEmbedding> results = await _aiVisionHelper.QuerySearchIndex(searchIndexName, queryVector);\n",
    "Console.WriteLine($\"Cognitive Search Results for '{question}':\"); \n",
    "foreach(ImageEmbedding result in results)\n",
    "{\n",
    "    Console.WriteLine($\"  - {result.ImageDescription}\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture sketch: \n",
    "\n",
    "![Embeddings](./img/Embeddings.png) \n",
    "\n",
    "is provided as top search result. Notice that in the sketch nowhere the phrase \"Vector\" is used. The MLLM conserved the semantic meaning of the architecture sketch and Embedding can be translated to Vector. Therefore the Azure AI Search instance provided it as result with the closest distance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
